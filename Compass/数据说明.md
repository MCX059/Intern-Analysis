# 指南者爬虫代码及数据字典

## 1. 代码
三个Jupyter Notebook文件是草稿，可以不用看。

BasicInfo.py是爬取每个案例对应的网址，因为原网站是动态加载的，所以用了selenium来模拟鼠标滑动。

Details.py爬取的是每个案例的具体信息，只用了BeautifulSoup和urllib。

## 2. 数据字典
一共存储了五张表，表的内容大致跟word里写的一样，有一些由于word里写的还需要讨论，所以暂时没有处理。目前得到的总案例数是6142个，基本上是网站上所有的案例。

#### （1）Pre

第一张表是Pre，记录了网站的原始分类，即案例编号、学校档次（985、211、普本、海本）、领域（工商理社）、地区（美港英新澳）和案例链接。把学校档次细分成清北、华五、中外合作可以但没必要，后期数据处理时再议。

#### （2）BasicInformation

第二张表是BasicInfromation，记录了基本信息，一共有23列如下：

|  表头             | 含义    | 数据类型 |
|  :----:          | :----:  |  :----:          |
| id               | 案例编号 | 整型 |
| title | 标题 | 字符串 |
| original_major | 本科专业 | 字符串 |
| original_school | 本科学校 | 字符串 |
| school_level | 本科学校层次 | 字符串 |
| admitted_major | 录取专业 | 字符串 |
| admitted_school | 录取学校 | 字符串 |
| location | 录取学校地区 | 字符串 |
| realm | 录取领域 | 字符串 |
| GPA | 绩点 | 浮点数 |
| toefl | 托福 | 整型 |
| ielts | 雅思 | 浮点数 |
| GRE | GRE | 整型 |
| GMAT | GMAT | 整型 |
| exp_sum | 总经历数 | 整型 |
| interns | 实习数 | 整型 |
| competition | 比赛数 | 整型 |
| program | 科研数 | 整型 |
| oversea | 海外数 | 整型 |
| interns_bool | 是否实习 | 整型 |
| competition_bool | 是否比赛 | 布尔 |
| program_bool | 是否科研 | 布尔 |
| oversea_bool | 有无海外 | 布尔 |

多了一个GMAT成绩，作为GRE成绩的补充。

对于录取学校和录取专业的缺失值，已经根据标题进行了相应填充，目前应当没有缺失值。

GPA分制和语言考试的数据处理暂时也没有做，专业对口也暂时没做。

#### （3）Admission

第三张表是Admission，记录了每个案例的录取信息，即案例编号、录取专业、录取学校、录取学校地区，以方便后续对学校及专业排名进行处理，合理的排名以及相应的人工处理需要后续分工。

#### （4）Background

第四张表是Background，记录了案例编号以及相对应的软性背景，以方便后期的文本分析。经过尝试，可以在有较大误差的前提下将实习公司分离出来，但是实习岗位可能不太方便。具体实现形式需要后续讨论。

#### （5）Err

第五张表是Err，相当于爬取过程中的日志，将有缺失数据（主要是背景和语言）的案例编号记录下来，供之后参考。

## 3. 注意事项

如果出现编码原因造成的乱码，可以用Excel导入提供的csv文件，导入时编码方式选择utf-8。但应该不会乱码。建议使用提供的sql文件导入MySQL数据库，方便后续的处理。

另外，如果希望自己试着爬取，可以按照数据字典先建立相应的表，然后修改Compass文件夹下面的两个py文件中数据库的内容即可。注意应先运行BasicInfo填完表Pre之后再运行Details。运行时间视网络情况大致为一个半小时左右。
